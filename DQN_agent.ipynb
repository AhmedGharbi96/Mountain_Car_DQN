{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\V02 With Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a DQN agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "        def __init__(self,state_size,action_size,path=None):\n",
    "            self.render=False\n",
    "\n",
    "            self.path=path\n",
    "            self.state_size=state_size\n",
    "            self.action_size=action_size\n",
    "\n",
    "            self.discount_factor=0.95\n",
    "            self.learning_rate=0.001\n",
    "            self.epsilon=1  #0.41#1\n",
    "            self.epsilon_decay=0.99999#0.99999\n",
    "            self.epsilon_min=0.01\n",
    "            self.batch_size=256\n",
    "            self.start_train=1000\n",
    "            self.memory=deque(maxlen=25000)\n",
    "            self.model=self.build_model()\n",
    "            self.frozen_target_model=self.build_model()\n",
    "\n",
    "            self.update_target_model()\n",
    "\n",
    "            self.update_counter=0\n",
    "\n",
    "            if (self.path!=None):\n",
    "                self.model.load_weights(self.path)\n",
    "                self.frozen_target_model.load_weights(self.path)\n",
    "        \n",
    "        def build_model(self):\n",
    "            model=Sequential()\n",
    "            model.add(Dense(50,input_shape=(self.state_size,),activation=\"relu\"))\n",
    "            #model.add(Dense(24,activation=\"relu\"))\n",
    "            model.add(Dense(self.action_size,activation=\"linear\"))\n",
    "            model.summary()\n",
    "            model.compile(loss='mean_squared_error',optimizer=Adam(lr=self.learning_rate))\n",
    "            return model\n",
    "        def update_target_model(self):\n",
    "            self.frozen_target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        def eps_greedy_policy(self,state):\n",
    "            if (self.epsilon<self.epsilon_min):\n",
    "                self.epsilon=self.epsilon_min\n",
    "            if (np.random.rand()<=self.epsilon):\n",
    "                return (random.randint(0,2))#((((1-(-1))*(random.random()-0))/(1-0))+(-1))\n",
    "            else :\n",
    "                state=np.reshape(state,(1,self.state_size))\n",
    "                q_value=self.model.predict(state)\n",
    "                return (np.argmax(q_value[0]))\n",
    "        def greedy_policy(self,state):\n",
    "            state=np.reshape(state,(1,self.state_size))\n",
    "            q_value=self.model.predict(state)\n",
    "            return (np.argmax(q_value[0]))\n",
    "        \n",
    "        def memorize_sample(self,state,action,reward,next_state,done):\n",
    "                self.memory.append((state,action,reward,next_state,done))\n",
    "                if len(self.memory)>10000:\n",
    "                    self.memory.popleft()\n",
    "                if (self.epsilon> self.epsilon_min):\n",
    "                    self.epsilon*=self.epsilon_decay\n",
    "        \n",
    "        def train_model(self):\n",
    "            if len(self.memory)< self.start_train:\n",
    "                return\n",
    "            mini_batch=random.sample(self.memory,self.batch_size)\n",
    "            first_state=np.zeros((self.batch_size,self.state_size))\n",
    "            next_state=np.zeros((self.batch_size,self.state_size))\n",
    "            actions,reward,done=[],[],[]\n",
    "            for i in range (self.batch_size):\n",
    "                first_state[i]=mini_batch[i][0]\n",
    "                actions.append(mini_batch[i][1])\n",
    "                reward.append(mini_batch[i][2])\n",
    "                next_state[i]=mini_batch[i][3]\n",
    "                done.append(mini_batch[i][4])\n",
    "            q_value=self.model.predict(first_state)\n",
    "            q_value_next=self.frozen_target_model.predict(next_state)\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                \"\"\"\n",
    "                if (next_state[i][0]):\n",
    "                    q_value[i][action[i]]= reward[i]\n",
    "                    self.update_counter+=1\n",
    "                else:\n",
    "                \"\"\"\n",
    "                q_value[i][actions[i]]= reward[i]+self.discount_factor*(np.amax(q_value_next[i]))\n",
    "                \n",
    "                \n",
    "            self.model.fit(first_state,q_value,batch_size=self.batch_size,epochs=1,verbose=0)\n",
    "            \"\"\"\n",
    "            if (self.update_counter%500==0):\n",
    "                self.update_target_model()\n",
    "            \"\"\"\n",
    "        \"\"\"\n",
    "        def load_weights(self):\n",
    "            if (self.path!=None):\n",
    "                self.model.load_weights(self.path)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode 0, mean reward=-0.2,epsilon=0.9980019886872552\n",
      "episode 1, mean reward=-1.0,epsilon=0.9960079694237177\n",
      "episode 2, mean reward=-1.0,epsilon=0.994017934233226\n",
      "episode 3, mean reward=-1.0,epsilon=0.9920318751555576\n",
      "episode 4, mean reward=-1.0,epsilon=0.9900497842463938\n",
      "episode 5, mean reward=-1.0,epsilon=0.9880716535772887\n",
      "episode 6, mean reward=-1.0,epsilon=0.9860974752356391\n",
      "episode 7, mean reward=-1.0,epsilon=0.9841272413246489\n",
      "episode 8, mean reward=-1.0,epsilon=0.9821609439633026\n",
      "episode 9, mean reward=-1.0,epsilon=0.980198575286327\n",
      "episode 10, mean reward=-1.0,epsilon=0.9782401274441684\n",
      "episode 11, mean reward=-1.0,epsilon=0.9762855926029546\n",
      "episode 12, mean reward=-1.0,epsilon=0.9743349629444651\n",
      "episode 13, mean reward=-1.0,epsilon=0.9723882306660994\n",
      "episode 14, mean reward=-1.0,epsilon=0.970445387980849\n",
      "episode 15, mean reward=-1.0,epsilon=0.9685064271172623\n",
      "episode 16, mean reward=-1.0,epsilon=0.9665713403194166\n",
      "episode 17, mean reward=-1.0,epsilon=0.9646401198468838\n",
      "episode 18, mean reward=-1.0,epsilon=0.9627127579747022\n",
      "episode 19, mean reward=-1.0,epsilon=0.9607892469933452\n",
      "episode 20, mean reward=-1.0,epsilon=0.9588695792086886\n",
      "episode 21, mean reward=-1.0,epsilon=0.9569537469419825\n",
      "episode 22, mean reward=-0.8,epsilon=0.9550417425298197\n",
      "episode 23, mean reward=-1.0,epsilon=0.9531335583241021\n",
      "episode 24, mean reward=-1.0,epsilon=0.9512291866920142\n",
      "episode 25, mean reward=-1.0,epsilon=0.9493286200159913\n",
      "episode 26, mean reward=-1.0,epsilon=0.947431850693687\n",
      "episode 27, mean reward=-1.0,epsilon=0.9455388711379469\n",
      "episode 28, mean reward=-1.0,epsilon=0.9436496737767741\n",
      "episode 29, mean reward=-1.0,epsilon=0.9417642510533004\n",
      "episode 30, mean reward=-1.0,epsilon=0.9398825954257575\n",
      "episode 31, mean reward=-1.0,epsilon=0.9380046993674448\n",
      "episode 32, mean reward=-1.0,epsilon=0.9361305553667009\n",
      "episode 33, mean reward=-1.0,epsilon=0.9342601559268722\n",
      "episode 34, mean reward=-1.0,epsilon=0.932393493566284\n",
      "episode 35, mean reward=-1.0,epsilon=0.9305305608182091\n",
      "episode 36, mean reward=-1.0,epsilon=0.9286713502308396\n",
      "episode 37, mean reward=-1.0,epsilon=0.9268158543672568\n",
      "episode 38, mean reward=-1.0,epsilon=0.9249640658054001\n",
      "episode 39, mean reward=-1.0,epsilon=0.9231159771380392\n",
      "episode 40, mean reward=-1.0,epsilon=0.9212715809727428\n",
      "episode 41, mean reward=-1.0,epsilon=0.9194308699318492\n",
      "episode 42, mean reward=-1.0,epsilon=0.9175938366524392\n",
      "episode 43, mean reward=-1.0,epsilon=0.9157604737863031\n",
      "episode 44, mean reward=-1.0,epsilon=0.9139307739999132\n",
      "episode 45, mean reward=-1.0,epsilon=0.912104729974396\n",
      "episode 46, mean reward=-1.0,epsilon=0.9102823344055\n",
      "episode 47, mean reward=-1.0,epsilon=0.9084635800035663\n",
      "episode 48, mean reward=-1.0,epsilon=0.9066484594935031\n",
      "episode 49, mean reward=-1.0,epsilon=0.9048369656147537\n",
      "episode 50, mean reward=-1.0,epsilon=0.9030290911212665\n",
      "episode 51, mean reward=-1.0,epsilon=0.9012248287814689\n",
      "episode 52, mean reward=-0.75,epsilon=0.8994241713782375\n",
      "episode 53, mean reward=-1.0,epsilon=0.8976271117088688\n",
      "episode 54, mean reward=-1.0,epsilon=0.8958336425850486\n",
      "episode 55, mean reward=-1.0,epsilon=0.8940437568328268\n",
      "episode 56, mean reward=-0.8,epsilon=0.8922574472925864\n",
      "episode 57, mean reward=-1.0,epsilon=0.8904747068190155\n",
      "episode 58, mean reward=-1.0,epsilon=0.8886955282810781\n",
      "episode 59, mean reward=-1.0,epsilon=0.886919904561988\n",
      "episode 60, mean reward=-1.0,epsilon=0.8851478285591758\n",
      "episode 61, mean reward=-1.0,epsilon=0.883379293184264\n",
      "episode 62, mean reward=-1.0,epsilon=0.8816142913630383\n",
      "episode 63, mean reward=-1.0,epsilon=0.8798528160354181\n",
      "episode 64, mean reward=-1.0,epsilon=0.878094860155429\n",
      "episode 65, mean reward=-1.0,epsilon=0.8763404166911752\n",
      "episode 66, mean reward=-1.0,epsilon=0.8745894786248111\n",
      "episode 67, mean reward=-1.0,epsilon=0.8728420389525116\n",
      "episode 68, mean reward=-1.0,epsilon=0.8710980906844452\n",
      "episode 69, mean reward=-1.0,epsilon=0.8693576268447476\n",
      "episode 70, mean reward=-1.0,epsilon=0.8676206404714909\n",
      "episode 71, mean reward=-1.0,epsilon=0.865887124616658\n",
      "episode 72, mean reward=-1.0,epsilon=0.8641570723461142\n",
      "episode 73, mean reward=-1.0,epsilon=0.862430476739578\n",
      "episode 74, mean reward=-1.0,epsilon=0.860707330890597\n",
      "episode 75, mean reward=-1.0,epsilon=0.8589876279065163\n",
      "episode 76, mean reward=-1.0,epsilon=0.8572713609084516\n",
      "episode 77, mean reward=-1.0,epsilon=0.8555585230312648\n",
      "episode 78, mean reward=-1.0,epsilon=0.8538491074235335\n",
      "episode 79, mean reward=-1.0,epsilon=0.8521431072475252\n",
      "episode 80, mean reward=-1.0,epsilon=0.8504405156791679\n",
      "episode 81, mean reward=-1.0,epsilon=0.8487413259080252\n",
      "episode 82, mean reward=-1.0,epsilon=0.8470455311372664\n",
      "episode 83, mean reward=-1.0,epsilon=0.8453531245836448\n",
      "episode 84, mean reward=-1.0,epsilon=0.8436640994774628\n",
      "episode 85, mean reward=-1.0,epsilon=0.8419784490625506\n",
      "episode 86, mean reward=-1.0,epsilon=0.8402961665962372\n",
      "episode 87, mean reward=-1.0,epsilon=0.8386172453493221\n",
      "episode 88, mean reward=-1.0,epsilon=0.8369416786060511\n",
      "episode 89, mean reward=-1.0,epsilon=0.835269459664089\n",
      "episode 90, mean reward=-1.0,epsilon=0.8336005818344904\n",
      "episode 91, mean reward=-1.0,epsilon=0.8319350384416746\n",
      "episode 92, mean reward=-1.0,epsilon=0.8302728228233995\n",
      "episode 93, mean reward=-1.0,epsilon=0.8286139283307342\n",
      "episode 94, mean reward=-1.0,epsilon=0.8269583483280317\n",
      "episode 95, mean reward=-1.0,epsilon=0.8253060761929046\n",
      "episode 96, mean reward=-1.0,epsilon=0.8236571053161951\n",
      "episode 97, mean reward=-1.0,epsilon=0.8220114291019515\n",
      "episode 98, mean reward=-1.0,epsilon=0.8203690409674015\n",
      "episode 99, mean reward=-1.0,epsilon=0.8187299343429234\n",
      "episode 100, mean reward=-0.75,epsilon=0.8170941026720239\n",
      "episode 101, mean reward=-1.0,epsilon=0.8154615394113077\n",
      "episode 102, mean reward=-1.0,epsilon=0.8138322380304562\n",
      "episode 103, mean reward=-1.0,epsilon=0.8122061920121953\n",
      "episode 104, mean reward=-1.0,epsilon=0.8105833948522739\n",
      "episode 105, mean reward=-1.0,epsilon=0.8089638400594364\n",
      "episode 106, mean reward=-1.0,epsilon=0.8073475211553969\n",
      "episode 107, mean reward=-1.0,epsilon=0.8057344316748124\n",
      "episode 108, mean reward=-1.0,epsilon=0.8041245651652585\n",
      "episode 109, mean reward=-1.0,epsilon=0.8025179151872026\n",
      "episode 110, mean reward=-1.0,epsilon=0.8009144753139792\n",
      "episode 111, mean reward=-1.0,epsilon=0.7993142391317618\n",
      "episode 112, mean reward=-1.0,epsilon=0.7977172002395393\n",
      "episode 113, mean reward=-1.0,epsilon=0.7961233522490894\n",
      "episode 114, mean reward=-1.0,epsilon=0.794532688784956\n",
      "episode 115, mean reward=-1.0,epsilon=0.7929452034844184\n",
      "episode 116, mean reward=-1.0,epsilon=0.7913608899974712\n",
      "episode 117, mean reward=-1.0,epsilon=0.7897797419867935\n",
      "episode 118, mean reward=-1.0,epsilon=0.7882017531277271\n",
      "episode 119, mean reward=-1.0,epsilon=0.7866269171082529\n",
      "episode 120, mean reward=-1.0,epsilon=0.785055227628962\n",
      "episode 121, mean reward=-1.0,epsilon=0.78348667840303\n",
      "episode 122, mean reward=-1.0,epsilon=0.7819212631561961\n",
      "episode 123, mean reward=-1.0,epsilon=0.7803589756267352\n",
      "episode 124, mean reward=-1.0,epsilon=0.7787998095654318\n",
      "episode 125, mean reward=-1.0,epsilon=0.7772437587355572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 126, mean reward=-1.0,epsilon=0.7756908169128434\n",
      "episode 127, mean reward=-1.0,epsilon=0.7741409778854593\n",
      "episode 128, mean reward=-1.0,epsilon=0.7725942354539854\n",
      "episode 129, mean reward=-1.0,epsilon=0.771050583431387\n",
      "episode 130, mean reward=-1.0,epsilon=0.7695100156429925\n",
      "episode 131, mean reward=-1.0,epsilon=0.7679725259264679\n",
      "episode 132, mean reward=-1.0,epsilon=0.7664381081317896\n",
      "episode 133, mean reward=-1.0,epsilon=0.7649067561212238\n",
      "episode 134, mean reward=-1.0,epsilon=0.7633784637692994\n",
      "episode 135, mean reward=-1.0,epsilon=0.7618532249627826\n",
      "episode 136, mean reward=-1.0,epsilon=0.7603310336006563\n",
      "episode 137, mean reward=-1.0,epsilon=0.7588118835940915\n",
      "episode 138, mean reward=-1.0,epsilon=0.7572957688664254\n",
      "episode 139, mean reward=-1.0,epsilon=0.7557826833531376\n",
      "episode 140, mean reward=-1.0,epsilon=0.7542726210018217\n",
      "episode 141, mean reward=-1.0,epsilon=0.7527655757721672\n",
      "episode 142, mean reward=-1.0,epsilon=0.7512615416359297\n",
      "episode 143, mean reward=-1.0,epsilon=0.749760512576911\n",
      "episode 144, mean reward=-1.0,epsilon=0.7482624825909339\n",
      "episode 145, mean reward=-1.0,epsilon=0.7467674456858147\n",
      "episode 146, mean reward=-1.0,epsilon=0.7452753958813457\n",
      "episode 147, mean reward=-1.0,epsilon=0.7437863272092646\n",
      "episode 148, mean reward=-1.0,epsilon=0.7423002337132366\n",
      "episode 149, mean reward=-1.0,epsilon=0.7408171094488248\n",
      "episode 150, mean reward=-1.0,epsilon=0.7393369484834712\n",
      "episode 151, mean reward=-1.0,epsilon=0.7378597448964705\n",
      "episode 152, mean reward=-1.0,epsilon=0.7363854927789492\n",
      "episode 153, mean reward=-1.0,epsilon=0.7349141862338359\n",
      "episode 154, mean reward=-1.0,epsilon=0.7334458193758445\n",
      "episode 155, mean reward=-1.0,epsilon=0.7319803863314468\n",
      "episode 156, mean reward=-1.0,epsilon=0.7305178812388504\n",
      "episode 157, mean reward=-1.0,epsilon=0.7290582982479732\n",
      "episode 158, mean reward=-1.0,epsilon=0.7276016315204241\n",
      "episode 159, mean reward=-1.0,epsilon=0.7261478752294744\n",
      "episode 160, mean reward=-1.0,epsilon=0.7246970235600398\n",
      "episode 161, mean reward=-1.0,epsilon=0.7232490707086549\n",
      "episode 162, mean reward=-1.0,epsilon=0.721804010883447\n",
      "episode 163, mean reward=-1.0,epsilon=0.7203618383041168\n",
      "episode 164, mean reward=-1.0,epsilon=0.7189225472019154\n",
      "episode 165, mean reward=-1.0,epsilon=0.7174861318196197\n",
      "episode 166, mean reward=-0.5,epsilon=0.7160525864115073\n",
      "episode 167, mean reward=-1.0,epsilon=0.7146219052433372\n",
      "episode 168, mean reward=-1.0,epsilon=0.7131940825923258\n",
      "episode 169, mean reward=-1.0,epsilon=0.7117691127471237\n",
      "episode 170, mean reward=-1.0,epsilon=0.7103469900077926\n",
      "episode 171, mean reward=-1.0,epsilon=0.7089277086857836\n",
      "episode 172, mean reward=-1.0,epsilon=0.7075112631039119\n",
      "episode 173, mean reward=-1.0,epsilon=0.7060976475963368\n",
      "episode 174, mean reward=-1.0,epsilon=0.7046868565085371\n",
      "episode 175, mean reward=-1.0,epsilon=0.7032788841972917\n",
      "episode 176, mean reward=-1.0,epsilon=0.7018737250306512\n",
      "episode 177, mean reward=-1.0,epsilon=0.7004713733879218\n",
      "episode 178, mean reward=-1.0,epsilon=0.6990718236596394\n",
      "episode 179, mean reward=-1.0,epsilon=0.697675070247546\n",
      "episode 180, mean reward=-1.0,epsilon=0.6962811075645717\n",
      "episode 181, mean reward=-1.0,epsilon=0.694889930034808\n",
      "episode 182, mean reward=-1.0,epsilon=0.6935015320934865\n",
      "episode 183, mean reward=-1.0,epsilon=0.6921159081869578\n",
      "episode 184, mean reward=-1.0,epsilon=0.6907330527726694\n",
      "episode 185, mean reward=-1.0,epsilon=0.689352960319143\n",
      "episode 186, mean reward=-1.0,epsilon=0.6879756253059515\n",
      "episode 187, mean reward=-1.0,epsilon=0.6866010422236978\n",
      "episode 188, mean reward=-1.0,epsilon=0.6852292055739919\n",
      "episode 189, mean reward=-1.0,epsilon=0.6838601098694327\n",
      "episode 190, mean reward=-1.0,epsilon=0.6824937496335783\n",
      "episode 191, mean reward=-1.0,epsilon=0.681130119400934\n",
      "episode 192, mean reward=-1.0,epsilon=0.679769213716921\n",
      "episode 193, mean reward=-1.0,epsilon=0.678411027137859\n",
      "episode 194, mean reward=-1.0,epsilon=0.6770555542309463\n",
      "episode 195, mean reward=-1.0,epsilon=0.6757027895742365\n",
      "episode 196, mean reward=-1.0,epsilon=0.6743527277566139\n",
      "episode 197, mean reward=-1.0,epsilon=0.6730053633777768\n",
      "episode 198, mean reward=-1.0,epsilon=0.6716606910482109\n",
      "episode 199, mean reward=-1.0,epsilon=0.6703187053891713\n",
      "episode 200, mean reward=-1.0,epsilon=0.6689794010326593\n",
      "episode 201, mean reward=-1.0,epsilon=0.6676427726214024\n",
      "episode 202, mean reward=-1.0,epsilon=0.6663088148088325\n",
      "episode 203, mean reward=-1.0,epsilon=0.6649775222590637\n",
      "episode 204, mean reward=-1.0,epsilon=0.6636488896468686\n",
      "episode 205, mean reward=-1.0,epsilon=0.6623229116576644\n",
      "episode 206, mean reward=-1.0,epsilon=0.6609995829874823\n",
      "episode 207, mean reward=-1.0,epsilon=0.6596788983429543\n",
      "episode 208, mean reward=-1.0,epsilon=0.6583608524412863\n",
      "episode 209, mean reward=-1.0,epsilon=0.6570454400102405\n",
      "episode 210, mean reward=-1.0,epsilon=0.6557326557881131\n",
      "episode 211, mean reward=-1.0,epsilon=0.6544224945237117\n",
      "episode 212, mean reward=-0.35,epsilon=0.6531149509763385\n",
      "episode 213, mean reward=-1.0,epsilon=0.6518100199157656\n",
      "episode 214, mean reward=-1.0,epsilon=0.6505076961222145\n",
      "episode 215, mean reward=-1.0,epsilon=0.6492079743863352\n",
      "episode 216, mean reward=-1.0,epsilon=0.6479108495091871\n",
      "episode 217, mean reward=-1.0,epsilon=0.6466163163022186\n",
      "episode 218, mean reward=-1.0,epsilon=0.6453243695872417\n",
      "episode 219, mean reward=-1.0,epsilon=0.6440350041964173\n",
      "episode 220, mean reward=-1.0,epsilon=0.6427482149722296\n",
      "episode 221, mean reward=-1.0,epsilon=0.6414639967674689\n",
      "episode 222, mean reward=-1.0,epsilon=0.6401823444452082\n",
      "episode 223, mean reward=-1.0,epsilon=0.6389032528787875\n",
      "episode 224, mean reward=-1.0,epsilon=0.6376267169517861\n",
      "episode 225, mean reward=-1.0,epsilon=0.6363527315580076\n",
      "episode 226, mean reward=-1.0,epsilon=0.6350812916014588\n",
      "episode 227, mean reward=-1.0,epsilon=0.6338123919963269\n",
      "episode 228, mean reward=-1.0,epsilon=0.6325460276669606\n",
      "episode 229, mean reward=-1.0,epsilon=0.631282193547851\n",
      "episode 230, mean reward=-1.0,epsilon=0.6300208845836076\n",
      "episode 231, mean reward=-1.0,epsilon=0.6287620957289447\n",
      "episode 232, mean reward=-1.0,epsilon=0.627505821948654\n",
      "episode 233, mean reward=-1.0,epsilon=0.626252058217587\n",
      "episode 234, mean reward=-1.0,epsilon=0.625000799520639\n",
      "episode 235, mean reward=-1.0,epsilon=0.6237520408527228\n",
      "episode 236, mean reward=-1.0,epsilon=0.6225057772187513\n",
      "episode 237, mean reward=-1.0,epsilon=0.6212620036336198\n",
      "episode 238, mean reward=-1.0,epsilon=0.6200207151221818\n",
      "episode 239, mean reward=-1.0,epsilon=0.6187819067192317\n",
      "episode 240, mean reward=-1.0,epsilon=0.6175455734694852\n",
      "episode 241, mean reward=-1.0,epsilon=0.616311710427558\n",
      "episode 242, mean reward=-1.0,epsilon=0.6150803126579463\n",
      "episode 243, mean reward=-1.0,epsilon=0.61385137523501\n",
      "episode 244, mean reward=-1.0,epsilon=0.6126248932429474\n",
      "episode 245, mean reward=-1.0,epsilon=0.6114008617757793\n",
      "episode 246, mean reward=-1.0,epsilon=0.6101792759373291\n",
      "episode 247, mean reward=-1.0,epsilon=0.6089601308412039\n",
      "episode 248, mean reward=-1.0,epsilon=0.607743421610773\n",
      "episode 249, mean reward=-1.0,epsilon=0.606529143379149\n",
      "episode 250, mean reward=-1.0,epsilon=0.6053172912891682\n",
      "episode 251, mean reward=-1.0,epsilon=0.6041078604933736\n",
      "episode 252, mean reward=-1.0,epsilon=0.6029008461539901\n",
      "episode 253, mean reward=-1.0,epsilon=0.6016962434429115\n",
      "episode 254, mean reward=-1.0,epsilon=0.6004940475416769\n",
      "episode 255, mean reward=-1.0,epsilon=0.599294253641453\n",
      "episode 256, mean reward=-1.0,epsilon=0.5980968569430148\n",
      "episode 257, mean reward=-1.0,epsilon=0.5969018526567264\n",
      "episode 258, mean reward=-1.0,epsilon=0.5957092360025199\n",
      "episode 259, mean reward=-1.0,epsilon=0.5945190022098814\n",
      "episode 260, mean reward=-1.0,epsilon=0.5933311465178254\n",
      "episode 261, mean reward=-1.0,epsilon=0.592145664174879\n",
      "episode 262, mean reward=-1.0,epsilon=0.5909625504390649\n",
      "episode 263, mean reward=-1.0,epsilon=0.5897818005778791\n",
      "episode 264, mean reward=-1.0,epsilon=0.5886034098682736\n",
      "episode 265, mean reward=-1.0,epsilon=0.5874273735966371\n",
      "episode 266, mean reward=-1.0,epsilon=0.5862536870587749\n",
      "episode 267, mean reward=-1.0,epsilon=0.5850823455598932\n",
      "episode 268, mean reward=-1.0,epsilon=0.5839133444145773\n",
      "episode 269, mean reward=-1.0,epsilon=0.5827466789467737\n",
      "episode 270, mean reward=-1.0,epsilon=0.5815823444897738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 271, mean reward=-1.0,epsilon=0.5804203363861905\n",
      "episode 272, mean reward=-1.0,epsilon=0.5792606499879438\n",
      "episode 273, mean reward=-1.0,epsilon=0.5781032806562408\n",
      "episode 274, mean reward=-1.0,epsilon=0.5769482237615552\n",
      "episode 275, mean reward=-1.0,epsilon=0.5757954746836118\n",
      "episode 276, mean reward=-1.0,epsilon=0.5746450288113678\n",
      "episode 277, mean reward=-1.0,epsilon=0.5734968815429914\n",
      "episode 278, mean reward=-1.0,epsilon=0.5723510282858457\n",
      "episode 279, mean reward=-1.0,epsilon=0.5712074644564694\n",
      "episode 280, mean reward=-1.0,epsilon=0.5700661854805614\n",
      "episode 281, mean reward=-1.0,epsilon=0.5689271867929586\n",
      "episode 282, mean reward=-1.0,epsilon=0.5677904638376176\n",
      "episode 283, mean reward=-1.0,epsilon=0.5666560120676015\n",
      "episode 284, mean reward=-1.0,epsilon=0.5655238269450559\n",
      "episode 285, mean reward=-1.0,epsilon=0.5643939039411923\n",
      "episode 286, mean reward=-1.0,epsilon=0.5632662385362731\n",
      "episode 287, mean reward=-1.0,epsilon=0.5621408262195919\n",
      "episode 288, mean reward=-1.0,epsilon=0.56101766248945\n",
      "episode 289, mean reward=-1.0,epsilon=0.5598967428531466\n",
      "episode 290, mean reward=-1.0,epsilon=0.5587780628269583\n",
      "episode 291, mean reward=-1.0,epsilon=0.557661617936117\n",
      "episode 292, mean reward=-1.0,epsilon=0.5565474037147962\n",
      "episode 293, mean reward=-1.0,epsilon=0.5554354157060956\n",
      "episode 294, mean reward=-1.0,epsilon=0.5543256494620156\n",
      "episode 295, mean reward=-1.0,epsilon=0.5532181005434466\n",
      "episode 296, mean reward=-1.0,epsilon=0.5521127645201457\n",
      "episode 297, mean reward=-0.85,epsilon=0.551009636970724\n",
      "episode 298, mean reward=-1.0,epsilon=0.5499087134826253\n",
      "episode 299, mean reward=-1.0,epsilon=0.5488099896521104\n",
      "episode 300, mean reward=-1.0,epsilon=0.5477134610842387\n",
      "episode 301, mean reward=-1.0,epsilon=0.5466191233928505\n",
      "episode 302, mean reward=-1.0,epsilon=0.545526972200549\n",
      "episode 303, mean reward=-1.0,epsilon=0.5444370031386854\n",
      "episode 304, mean reward=-1.0,epsilon=0.5433492118473379\n",
      "episode 305, mean reward=-1.0,epsilon=0.5422635939752961\n",
      "episode 306, mean reward=-1.0,epsilon=0.541180145180044\n",
      "episode 307, mean reward=-1.0,epsilon=0.5400988611277417\n",
      "episode 308, mean reward=-1.0,epsilon=0.5390197374932081\n",
      "episode 309, mean reward=-1.0,epsilon=0.5379427699599039\n",
      "episode 310, mean reward=-1.0,epsilon=0.5368679542199153\n",
      "episode 311, mean reward=-0.8,epsilon=0.5357952859739339\n",
      "episode 312, mean reward=-0.7,epsilon=0.5347247609312433\n",
      "episode 313, mean reward=-0.45,epsilon=0.5336563748096985\n",
      "episode 314, mean reward=-1.0,epsilon=0.5325901233357102\n",
      "episode 315, mean reward=-1.0,epsilon=0.5315260022442297\n",
      "episode 316, mean reward=-1.0,epsilon=0.5304640072787289\n",
      "episode 317, mean reward=-1.0,epsilon=0.5294041341911827\n",
      "episode 318, mean reward=-1.0,epsilon=0.5283463787420543\n",
      "episode 319, mean reward=-1.0,epsilon=0.52729073670028\n",
      "episode 320, mean reward=-1.0,epsilon=0.5262372038432472\n",
      "episode 321, mean reward=-1.0,epsilon=0.5251857759567812\n",
      "episode 322, mean reward=-1.0,epsilon=0.5241364488351263\n",
      "episode 323, mean reward=-1.0,epsilon=0.5230892182809327\n",
      "episode 324, mean reward=-1.0,epsilon=0.5220440801052326\n",
      "episode 325, mean reward=-1.0,epsilon=0.5210010301274313\n",
      "episode 326, mean reward=-1.0,epsilon=0.5199600641752858\n",
      "episode 327, mean reward=-1.0,epsilon=0.5189211780848889\n",
      "episode 328, mean reward=-1.0,epsilon=0.517884367700653\n",
      "episode 329, mean reward=-1.0,epsilon=0.5168496288752937\n",
      "episode 330, mean reward=-1.0,epsilon=0.5158169574698126\n",
      "episode 331, mean reward=-1.0,epsilon=0.5147863493534828\n",
      "episode 332, mean reward=-1.0,epsilon=0.5137578004038285\n",
      "episode 333, mean reward=-1.0,epsilon=0.5127313065066106\n",
      "episode 334, mean reward=-1.0,epsilon=0.5117068635558124\n",
      "episode 335, mean reward=-1.0,epsilon=0.5106844674536193\n",
      "episode 336, mean reward=-1.0,epsilon=0.509664114110404\n",
      "episode 337, mean reward=-1.0,epsilon=0.5086457994447107\n",
      "episode 338, mean reward=-1.0,epsilon=0.50762951938324\n",
      "episode 339, mean reward=-1.0,epsilon=0.5066152698608287\n",
      "episode 340, mean reward=-1.0,epsilon=0.5056030468204376\n",
      "episode 341, mean reward=-1.0,epsilon=0.5045928462131323\n",
      "episode 342, mean reward=-1.0,epsilon=0.503584663998069\n",
      "episode 343, mean reward=-1.0,epsilon=0.5025784961424766\n",
      "episode 344, mean reward=-1.0,epsilon=0.5015743386216417\n",
      "episode 345, mean reward=-1.0,epsilon=0.5005721874188939\n",
      "episode 346, mean reward=-1.0,epsilon=0.49957203852558585\n",
      "episode 347, mean reward=-1.0,epsilon=0.4985738879410804\n",
      "episode 348, mean reward=-1.0,epsilon=0.49757773167273495\n",
      "episode 349, mean reward=-1.0,epsilon=0.4965835657358831\n",
      "episode 350, mean reward=-1.0,epsilon=0.49559138615382026\n",
      "episode 351, mean reward=-1.0,epsilon=0.4946011889577864\n",
      "episode 352, mean reward=-1.0,epsilon=0.49361297018695216\n",
      "episode 353, mean reward=-1.0,epsilon=0.4926267258884014\n",
      "episode 354, mean reward=-1.0,epsilon=0.4916424521171163\n",
      "episode 355, mean reward=-1.0,epsilon=0.4906601449359609\n",
      "episode 356, mean reward=-1.0,epsilon=0.48967980041566606\n",
      "episode 357, mean reward=-1.0,epsilon=0.4887014146348132\n",
      "episode 358, mean reward=-1.0,epsilon=0.4877249836798188\n",
      "episode 359, mean reward=-1.0,epsilon=0.48675050364491823\n",
      "episode 360, mean reward=-1.0,epsilon=0.485777970632152\n",
      "episode 361, mean reward=-1.0,epsilon=0.48480738075134705\n",
      "episode 362, mean reward=-1.0,epsilon=0.48383873012010387\n",
      "episode 363, mean reward=-1.0,epsilon=0.48287201486377995\n",
      "episode 364, mean reward=-1.0,epsilon=0.4819072311154746\n",
      "episode 365, mean reward=-1.0,epsilon=0.48094437501601195\n",
      "episode 366, mean reward=-1.0,epsilon=0.4799834427139294\n",
      "episode 367, mean reward=-1.0,epsilon=0.47902443036545683\n",
      "episode 368, mean reward=-1.0,epsilon=0.4780673341345057\n",
      "episode 369, mean reward=-1.0,epsilon=0.47711215019265096\n",
      "episode 370, mean reward=-1.0,epsilon=0.4761588747191185\n",
      "episode 371, mean reward=-0.7,epsilon=0.4752075039007662\n",
      "episode 372, mean reward=-1.0,epsilon=0.47425803393207144\n",
      "episode 373, mean reward=-1.0,epsilon=0.4733104610151154\n",
      "episode 374, mean reward=-1.0,epsilon=0.4723647813595669\n",
      "episode 375, mean reward=-1.0,epsilon=0.47142099118266895\n",
      "episode 376, mean reward=-1.0,epsilon=0.47047908670922084\n",
      "episode 377, mean reward=-1.0,epsilon=0.4695390641715665\n",
      "episode 378, mean reward=-1.0,epsilon=0.4686009198095767\n",
      "episode 379, mean reward=-1.0,epsilon=0.46766464987063483\n",
      "episode 380, mean reward=-1.0,epsilon=0.46673025060962275\n",
      "episode 381, mean reward=-1.0,epsilon=0.46579771828890487\n",
      "episode 382, mean reward=-1.0,epsilon=0.46486704917831306\n",
      "episode 383, mean reward=-1.0,epsilon=0.4639382395551328\n",
      "episode 384, mean reward=-1.0,epsilon=0.463011285704087\n",
      "episode 385, mean reward=-1.0,epsilon=0.46208618391732187\n",
      "episode 386, mean reward=-1.0,epsilon=0.46116293049439194\n",
      "episode 387, mean reward=-1.0,epsilon=0.46024152174224536\n",
      "episode 388, mean reward=-1.0,epsilon=0.45932195397521\n",
      "episode 389, mean reward=-1.0,epsilon=0.45840422351497584\n",
      "episode 390, mean reward=-1.0,epsilon=0.4574883266905832\n",
      "episode 391, mean reward=-1.0,epsilon=0.4565742598384067\n",
      "episode 392, mean reward=-1.0,epsilon=0.45566201930214156\n",
      "episode 393, mean reward=-1.0,epsilon=0.4547516014327877\n",
      "episode 394, mean reward=-1.0,epsilon=0.45384300258863663\n",
      "episode 395, mean reward=-1.0,epsilon=0.4529362191352549\n",
      "episode 396, mean reward=-1.0,epsilon=0.45203124744547135\n",
      "episode 397, mean reward=-1.0,epsilon=0.4511280838993613\n",
      "episode 398, mean reward=-1.0,epsilon=0.45022672488423354\n",
      "episode 399, mean reward=-1.0,epsilon=0.44932716679461515\n",
      "episode 400, mean reward=-1.0,epsilon=0.44842940603223613\n",
      "episode 401, mean reward=-1.0,epsilon=0.4475334390060166\n",
      "episode 402, mean reward=-1.0,epsilon=0.44663926213205146\n",
      "episode 403, mean reward=-1.0,epsilon=0.4457468718335958\n",
      "episode 404, mean reward=-1.0,epsilon=0.4448562645410515\n",
      "episode 405, mean reward=-1.0,epsilon=0.4439674366919536\n",
      "episode 406, mean reward=-1.0,epsilon=0.4430803847309525\n",
      "episode 407, mean reward=-1.0,epsilon=0.44219510510980486\n",
      "episode 408, mean reward=-1.0,epsilon=0.4413115942873551\n",
      "episode 409, mean reward=-1.0,epsilon=0.44042984872952373\n",
      "episode 410, mean reward=-1.0,epsilon=0.43954986490929215\n",
      "episode 411, mean reward=-1.0,epsilon=0.43867163930668795\n",
      "episode 412, mean reward=-1.0,epsilon=0.4377951684087732\n",
      "episode 413, mean reward=-1.0,epsilon=0.4369204487096276\n",
      "episode 414, mean reward=-1.0,epsilon=0.436047476710336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 415, mean reward=-1.0,epsilon=0.43517624891897544\n",
      "episode 416, mean reward=-1.0,epsilon=0.43430676185059747\n",
      "episode 417, mean reward=-1.0,epsilon=0.43343901202721846\n",
      "episode 418, mean reward=-1.0,epsilon=0.43257299597780324\n",
      "episode 419, mean reward=-1.0,epsilon=0.43170871023825186\n",
      "episode 420, mean reward=-1.0,epsilon=0.4308461513513858\n",
      "episode 421, mean reward=-1.0,epsilon=0.42998531586693334\n",
      "episode 422, mean reward=-1.0,epsilon=0.4291262003415175\n",
      "episode 423, mean reward=-1.0,epsilon=0.4282688013386398\n",
      "episode 424, mean reward=-1.0,epsilon=0.4274131154286699\n",
      "episode 425, mean reward=-1.0,epsilon=0.4265591391888281\n",
      "episode 426, mean reward=-1.0,epsilon=0.4257068692031748\n",
      "episode 427, mean reward=-1.0,epsilon=0.42485630206259395\n",
      "episode 428, mean reward=-1.0,epsilon=0.4240074343647822\n",
      "episode 429, mean reward=-0.85,epsilon=0.4231602627142337\n",
      "episode 430, mean reward=-1.0,epsilon=0.4223147837222271\n",
      "episode 431, mean reward=-1.0,epsilon=0.4214709940068111\n",
      "episode 432, mean reward=-1.0,epsilon=0.4206288901927922\n",
      "episode 433, mean reward=-1.0,epsilon=0.41978846891171967\n",
      "episode 434, mean reward=-1.0,epsilon=0.4189497268018744\n",
      "episode 435, mean reward=-1.0,epsilon=0.4181126605082532\n",
      "episode 436, mean reward=-1.0,epsilon=0.4172772666825562\n",
      "episode 437, mean reward=-1.0,epsilon=0.41644354198317335\n",
      "episode 438, mean reward=-1.0,epsilon=0.41561148307517176\n",
      "episode 439, mean reward=-1.0,epsilon=0.4147810866302807\n",
      "episode 440, mean reward=-1.0,epsilon=0.41395234932688135\n",
      "episode 441, mean reward=-1.0,epsilon=0.4131252678499892\n",
      "episode 442, mean reward=-1.0,epsilon=0.4122998388912444\n",
      "episode 443, mean reward=-1.0,epsilon=0.41147605914889673\n",
      "episode 444, mean reward=-1.0,epsilon=0.41065392532779377\n",
      "episode 445, mean reward=-1.0,epsilon=0.4098334341393658\n",
      "episode 446, mean reward=-1.0,epsilon=0.4090145823016141\n",
      "episode 447, mean reward=-1.0,epsilon=0.4081973665390979\n",
      "episode 448, mean reward=-1.0,epsilon=0.4073817835829203\n",
      "episode 449, mean reward=-1.0,epsilon=0.4065678301707155\n",
      "episode 450, mean reward=-1.0,epsilon=0.4057555030466364\n",
      "episode 451, mean reward=-1.0,epsilon=0.4049447989613406\n",
      "episode 452, mean reward=-1.0,epsilon=0.40413571467197934\n",
      "episode 453, mean reward=-1.0,epsilon=0.40332824694218095\n",
      "episode 454, mean reward=-1.0,epsilon=0.40252239254204125\n",
      "episode 455, mean reward=-1.0,epsilon=0.40171814824810953\n",
      "episode 456, mean reward=-1.0,epsilon=0.4009155108433754\n",
      "episode 457, mean reward=-1.0,epsilon=0.40011447711725556\n",
      "episode 458, mean reward=-1.0,epsilon=0.39931504386558253\n",
      "episode 459, mean reward=-1.0,epsilon=0.39851720789058975\n",
      "episode 460, mean reward=1.75,epsilon=0.39772096600090145\n",
      "episode 461, mean reward=5.5,epsilon=0.39692631501151643\n",
      "episode 462, mean reward=-1.0,epsilon=0.3961332517437971\n",
      "episode 463, mean reward=-1.0,epsilon=0.39534177302545914\n",
      "episode 464, mean reward=13.0,epsilon=0.39455187569055383\n",
      "episode 465, mean reward=-1.0,epsilon=0.3937635565794596\n",
      "episode 466, mean reward=-1.0,epsilon=0.3929768125388675\n",
      "episode 467, mean reward=20.0,epsilon=0.3921916404217688\n",
      "episode 468, mean reward=-1.0,epsilon=0.39140803708744254\n",
      "episode 469, mean reward=-1.0,epsilon=0.3906259994014428\n",
      "episode 470, mean reward=20.0,epsilon=0.38984552423558655\n",
      "episode 471, mean reward=-1.0,epsilon=0.38906660846794106\n",
      "episode 472, mean reward=-1.0,epsilon=0.3882892489828112\n",
      "episode 473, mean reward=23.0,epsilon=0.38751344267072657\n",
      "episode 474, mean reward=-1.0,epsilon=0.3867391864284302\n",
      "episode 475, mean reward=-1.0,epsilon=0.38596647715886473\n",
      "episode 476, mean reward=-1.0,epsilon=0.38519531177116123\n",
      "episode 477, mean reward=-1.0,epsilon=0.3844256871806262\n",
      "episode 478, mean reward=-1.0,epsilon=0.38365760030872986\n",
      "episode 479, mean reward=4.0,epsilon=0.38289104808309254\n",
      "episode 480, mean reward=-1.0,epsilon=0.38212602743747387\n",
      "episode 481, mean reward=-1.0,epsilon=0.3813625353117598\n",
      "episode 482, mean reward=-1.0,epsilon=0.38060056865195\n",
      "episode 483, mean reward=7.5,epsilon=0.37984012441014664\n",
      "episode 484, mean reward=-1.0,epsilon=0.37908119954454067\n",
      "episode 485, mean reward=-1.0,epsilon=0.37832379101940194\n",
      "episode 486, mean reward=-1.0,epsilon=0.3775678958050648\n",
      "episode 487, mean reward=-1.0,epsilon=0.37681351087791715\n",
      "episode 488, mean reward=-1.0,epsilon=0.37606063322038813\n",
      "episode 489, mean reward=20.5,epsilon=0.3753092598209357\n",
      "episode 490, mean reward=36.5,epsilon=0.37455938767403596\n",
      "episode 491, mean reward=-1.0,epsilon=0.37381101378016857\n",
      "episode 492, mean reward=-1.0,epsilon=0.37306413514580794\n",
      "episode 493, mean reward=-0.5,epsilon=0.37231874878340754\n",
      "episode 494, mean reward=-1.0,epsilon=0.3715748517113911\n",
      "episode 495, mean reward=-1.0,epsilon=0.3708324409541405\n",
      "episode 496, mean reward=-1.0,epsilon=0.37009151354198133\n",
      "episode 497, mean reward=-1.0,epsilon=0.3693520665111741\n",
      "episode 498, mean reward=-1.0,epsilon=0.368614096903899\n",
      "episode 499, mean reward=7.5,epsilon=0.3678776017682482\n",
      "episode 500, mean reward=31.0,epsilon=0.3671425781582098\n",
      "episode 501, mean reward=-1.0,epsilon=0.3664090231336599\n",
      "episode 502, mean reward=3.0,epsilon=0.3656769337603466\n",
      "episode 503, mean reward=-1.0,epsilon=0.3649463071098839\n",
      "episode 504, mean reward=20.0,epsilon=0.3642171402597344\n",
      "episode 505, mean reward=-1.0,epsilon=0.36348943029320013\n",
      "episode 506, mean reward=2.5,epsilon=0.36276317429941124\n",
      "episode 507, mean reward=7.5,epsilon=0.36203836937331424\n",
      "episode 508, mean reward=11.5,epsilon=0.3613150126156592\n",
      "episode 509, mean reward=-1.0,epsilon=0.3605931011329881\n",
      "episode 510, mean reward=-1.0,epsilon=0.35987263203762676\n",
      "episode 511, mean reward=16.0,epsilon=0.35915360244766836\n",
      "episode 512, mean reward=-1.0,epsilon=0.358436009486965\n",
      "episode 513, mean reward=-1.0,epsilon=0.35771985028511516\n",
      "episode 514, mean reward=8.0,epsilon=0.3570051219774521\n",
      "episode 515, mean reward=19.0,epsilon=0.3562918217050334\n",
      "episode 516, mean reward=-1.0,epsilon=0.35557994661462844\n",
      "episode 517, mean reward=22.0,epsilon=0.3548694938587075\n",
      "episode 518, mean reward=17.5,epsilon=0.3541604605954303\n",
      "episode 519, mean reward=-1.0,epsilon=0.35345284398863397\n",
      "episode 520, mean reward=-1.0,epsilon=0.352746641207823\n",
      "episode 521, mean reward=17.0,epsilon=0.35204184942815714\n",
      "episode 522, mean reward=23.5,epsilon=0.35133846583044004\n",
      "episode 523, mean reward=15.5,epsilon=0.35063648760110844\n",
      "episode 524, mean reward=5.0,epsilon=0.34993591193222057\n",
      "episode 525, mean reward=23.0,epsilon=0.3492367360214448\n",
      "episode 526, mean reward=1.0,epsilon=0.3485389570720484\n",
      "episode 527, mean reward=1.0,epsilon=0.3478425722928863\n",
      "episode 528, mean reward=-1.0,epsilon=0.34714757889839126\n",
      "episode 529, mean reward=13.5,epsilon=0.34645397410856066\n",
      "episode 530, mean reward=-1.0,epsilon=0.3457617551489464\n",
      "episode 531, mean reward=19.5,epsilon=0.34507091925064415\n",
      "episode 532, mean reward=-1.0,epsilon=0.34438146365028227\n",
      "episode 533, mean reward=-1.0,epsilon=0.3436933855900092\n",
      "episode 534, mean reward=-1.0,epsilon=0.34300668231748466\n",
      "episode 535, mean reward=17.0,epsilon=0.3423213510858675\n",
      "episode 536, mean reward=-1.0,epsilon=0.34163738915380426\n",
      "episode 537, mean reward=11.5,epsilon=0.3409547937854188\n",
      "episode 538, mean reward=-0.5,epsilon=0.34027356225030075\n",
      "episode 539, mean reward=-1.0,epsilon=0.3395936918234972\n",
      "episode 540, mean reward=-1.0,epsilon=0.33891517978549734\n",
      "episode 541, mean reward=18.0,epsilon=0.3382380234222251\n",
      "episode 542, mean reward=-1.0,epsilon=0.33756222002502734\n",
      "episode 543, mean reward=18.0,epsilon=0.336887766890662\n",
      "episode 544, mean reward=16.5,epsilon=0.33621466132128963\n",
      "episode 545, mean reward=-1.0,epsilon=0.3355429006244595\n",
      "episode 546, mean reward=-1.0,epsilon=0.3348724821131009\n",
      "episode 547, mean reward=12.0,epsilon=0.3342034031055121\n",
      "episode 548, mean reward=-1.0,epsilon=0.33353566092534925\n",
      "episode 549, mean reward=-1.0,epsilon=0.3328692529016169\n",
      "episode 550, mean reward=-1.0,epsilon=0.33220417636865524\n",
      "episode 551, mean reward=-1.0,epsilon=0.33154042866612987\n",
      "episode 552, mean reward=18.5,epsilon=0.3308780071390228\n",
      "episode 553, mean reward=4.5,epsilon=0.3302169091376212\n",
      "episode 554, mean reward=-1.0,epsilon=0.3295571320175043\n",
      "episode 555, mean reward=-1.0,epsilon=0.3288986731395381\n",
      "episode 556, mean reward=-1.0,epsilon=0.3282415298698594\n",
      "episode 557, mean reward=-1.0,epsilon=0.3275856995798668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 558, mean reward=-1.0,epsilon=0.326931179646213\n",
      "episode 559, mean reward=-1.0,epsilon=0.3262779674507909\n",
      "episode 560, mean reward=-1.0,epsilon=0.3256260603807251\n",
      "episode 561, mean reward=-1.0,epsilon=0.3249754558283598\n",
      "episode 562, mean reward=-1.0,epsilon=0.32432615119125024\n",
      "episode 563, mean reward=-1.0,epsilon=0.32367814387215094\n",
      "episode 564, mean reward=-1.0,epsilon=0.3230314312790066\n",
      "episode 565, mean reward=-1.0,epsilon=0.3223860108249391\n",
      "episode 566, mean reward=-1.0,epsilon=0.32174187992824016\n",
      "episode 567, mean reward=-1.0,epsilon=0.32109903601235984\n",
      "episode 568, mean reward=-1.0,epsilon=0.3204574765058953\n",
      "episode 569, mean reward=-1.0,epsilon=0.31981719884258314\n",
      "episode 570, mean reward=-1.0,epsilon=0.3191782004612854\n",
      "episode 571, mean reward=-1.0,epsilon=0.3185404788059825\n",
      "episode 572, mean reward=-1.0,epsilon=0.3179040313257608\n",
      "episode 573, mean reward=-1.0,epsilon=0.31726885547480493\n",
      "episode 574, mean reward=-1.0,epsilon=0.31663494871238457\n",
      "episode 575, mean reward=-1.0,epsilon=0.31600230850284716\n",
      "episode 576, mean reward=-1.0,epsilon=0.31537093231560526\n",
      "episode 577, mean reward=-1.0,epsilon=0.3147408176251284\n",
      "episode 578, mean reward=-1.0,epsilon=0.31411196191093105\n",
      "episode 579, mean reward=-1.0,epsilon=0.3134843626575645\n",
      "episode 580, mean reward=-1.0,epsilon=0.31285801735460594\n",
      "episode 581, mean reward=-1.0,epsilon=0.3122329234966486\n",
      "episode 582, mean reward=-1.0,epsilon=0.31160907858329107\n",
      "episode 583, mean reward=-1.0,epsilon=0.31098648011912805\n",
      "episode 584, mean reward=-1.0,epsilon=0.31036512561373913\n",
      "episode 585, mean reward=-1.0,epsilon=0.30974501258168174\n",
      "episode 586, mean reward=-1.0,epsilon=0.30912613854247734\n",
      "episode 587, mean reward=-1.0,epsilon=0.3085085010206044\n",
      "episode 588, mean reward=-1.0,epsilon=0.30789209754548713\n",
      "episode 589, mean reward=-1.0,epsilon=0.3072769256514861\n",
      "episode 590, mean reward=-1.0,epsilon=0.30666298287788973\n",
      "episode 591, mean reward=-1.0,epsilon=0.30605026676889985\n",
      "episode 592, mean reward=-1.0,epsilon=0.30543877487362675\n",
      "episode 593, mean reward=-1.0,epsilon=0.3048285047460786\n",
      "episode 594, mean reward=-1.0,epsilon=0.30421945394514904\n",
      "episode 595, mean reward=-1.0,epsilon=0.30361162003460984\n",
      "episode 596, mean reward=-1.0,epsilon=0.30300500058310026\n",
      "episode 597, mean reward=-1.0,epsilon=0.30239959316411696\n",
      "episode 598, mean reward=-1.0,epsilon=0.30179539535600586\n",
      "episode 599, mean reward=-1.0,epsilon=0.3011924047419503\n",
      "episode 600, mean reward=-1.0,epsilon=0.30059061890996314\n",
      "episode 601, mean reward=-1.0,epsilon=0.299990035452876\n",
      "episode 602, mean reward=-1.0,epsilon=0.29939065196833065\n",
      "episode 603, mean reward=-1.0,epsilon=0.29879246605876814\n",
      "episode 604, mean reward=-1.0,epsilon=0.29819547533141966\n",
      "episode 605, mean reward=-1.0,epsilon=0.2975996773982985\n",
      "episode 606, mean reward=-1.0,epsilon=0.2970050698761876\n",
      "episode 607, mean reward=-1.0,epsilon=0.2964116503866328\n",
      "episode 608, mean reward=-1.0,epsilon=0.2958194165559309\n",
      "episode 609, mean reward=-1.0,epsilon=0.29522836601512276\n",
      "episode 610, mean reward=-1.0,epsilon=0.29463849639998096\n",
      "episode 611, mean reward=-1.0,epsilon=0.2940498053510039\n",
      "episode 612, mean reward=-1.0,epsilon=0.2934622905134022\n",
      "episode 613, mean reward=-1.0,epsilon=0.29287594953709284\n",
      "episode 614, mean reward=-1.0,epsilon=0.29229078007668685\n",
      "episode 615, mean reward=-1.0,epsilon=0.291706779791483\n",
      "episode 616, mean reward=-1.0,epsilon=0.29112394634545535\n",
      "episode 617, mean reward=-1.0,epsilon=0.29054227740724636\n",
      "episode 618, mean reward=-1.0,epsilon=0.2899617706501562\n",
      "episode 619, mean reward=-1.0,epsilon=0.2893824237521339\n",
      "episode 620, mean reward=-1.0,epsilon=0.2888042343957678\n",
      "episode 621, mean reward=-1.0,epsilon=0.2882272002682766\n",
      "episode 622, mean reward=-1.0,epsilon=0.2876513190615\n",
      "episode 623, mean reward=-1.0,epsilon=0.2870765884718895\n",
      "episode 624, mean reward=-1.0,epsilon=0.28650300620049846\n",
      "episode 625, mean reward=-1.0,epsilon=0.2859305699529749\n",
      "episode 626, mean reward=-1.0,epsilon=0.28535927743954925\n",
      "episode 627, mean reward=-1.0,epsilon=0.2847891263750283\n",
      "episode 628, mean reward=-1.0,epsilon=0.2842201144787845\n",
      "episode 629, mean reward=-1.0,epsilon=0.28365223947474627\n",
      "episode 630, mean reward=-1.0,epsilon=0.28308549909139014\n",
      "episode 631, mean reward=-1.0,epsilon=0.2825198910617316\n",
      "episode 632, mean reward=-1.0,epsilon=0.28195541312331523\n",
      "episode 633, mean reward=-1.0,epsilon=0.2813920630182051\n",
      "episode 634, mean reward=-1.0,epsilon=0.28082983849297816\n",
      "episode 635, mean reward=-1.0,epsilon=0.28026873729871327\n",
      "episode 636, mean reward=-1.0,epsilon=0.2797087571909821\n",
      "episode 637, mean reward=-1.0,epsilon=0.2791498959298404\n",
      "episode 638, mean reward=-1.0,epsilon=0.2785921512798211\n",
      "episode 639, mean reward=-1.0,epsilon=0.27803552100992185\n",
      "episode 640, mean reward=-1.0,epsilon=0.2774800028935994\n",
      "episode 641, mean reward=-1.0,epsilon=0.2769255947087579\n",
      "episode 642, mean reward=-1.0,epsilon=0.2763722942377411\n",
      "episode 643, mean reward=-1.0,epsilon=0.2758200992673249\n",
      "episode 644, mean reward=-1.0,epsilon=0.27526900758870637\n",
      "episode 645, mean reward=-1.0,epsilon=0.2747190169974964\n",
      "episode 646, mean reward=-1.0,epsilon=0.27417012529370915\n",
      "episode 647, mean reward=-1.0,epsilon=0.2736223302817554\n",
      "episode 648, mean reward=-1.0,epsilon=0.27307562977043304\n",
      "episode 649, mean reward=12.5,epsilon=0.27253002157291667\n",
      "episode 650, mean reward=-1.0,epsilon=0.2719855035067515\n",
      "episode 651, mean reward=-1.0,epsilon=0.2714420733938425\n",
      "episode 652, mean reward=-1.0,epsilon=0.27089972906044674\n",
      "episode 653, mean reward=-1.0,epsilon=0.2703584683371642\n",
      "episode 654, mean reward=-1.0,epsilon=0.2698182890589307\n",
      "episode 655, mean reward=-1.0,epsilon=0.2692791890650057\n",
      "episode 656, mean reward=-1.0,epsilon=0.26874116619896715\n",
      "episode 657, mean reward=-1.0,epsilon=0.26820421830870184\n",
      "episode 658, mean reward=-1.0,epsilon=0.26766834324639555\n",
      "episode 659, mean reward=-1.0,epsilon=0.2671335388685256\n",
      "episode 660, mean reward=-1.0,epsilon=0.2665998030358529\n",
      "episode 661, mean reward=-1.0,epsilon=0.26606713361341205\n",
      "episode 662, mean reward=-1.0,epsilon=0.26553552847050305\n",
      "episode 663, mean reward=-1.0,epsilon=0.26500498548068363\n",
      "episode 664, mean reward=-1.0,epsilon=0.2644755025217599\n",
      "episode 665, mean reward=-1.0,epsilon=0.2639470774757776\n",
      "episode 666, mean reward=-1.0,epsilon=0.26341970822901517\n",
      "episode 667, mean reward=-1.0,epsilon=0.26289339267197365\n",
      "episode 668, mean reward=-1.0,epsilon=0.262368128699369\n",
      "episode 669, mean reward=-1.0,epsilon=0.2618439142101241\n",
      "episode 670, mean reward=-1.0,epsilon=0.2613207471073588\n",
      "episode 671, mean reward=-1.0,epsilon=0.2607986252983834\n",
      "episode 672, mean reward=-1.0,epsilon=0.2602775466946892\n",
      "episode 673, mean reward=-1.0,epsilon=0.2597575092119399\n",
      "episode 674, mean reward=-1.0,epsilon=0.2592385107699642\n",
      "episode 675, mean reward=-1.0,epsilon=0.2587205492927467\n",
      "episode 676, mean reward=-1.0,epsilon=0.2582036227084201\n",
      "episode 677, mean reward=-1.0,epsilon=0.2576877289492572\n",
      "episode 678, mean reward=-1.0,epsilon=0.25717286595166117\n",
      "episode 679, mean reward=-1.0,epsilon=0.25665903165615905\n",
      "episode 680, mean reward=-1.0,epsilon=0.2561462240073922\n",
      "episode 681, mean reward=-1.0,epsilon=0.2556344409541088\n",
      "episode 682, mean reward=-1.0,epsilon=0.2551236804491557\n",
      "episode 683, mean reward=-1.0,epsilon=0.2546139404494691\n",
      "episode 684, mean reward=-1.0,epsilon=0.25410521891606896\n",
      "episode 685, mean reward=-1.0,epsilon=0.2535975138140477\n",
      "episode 686, mean reward=-1.0,epsilon=0.2530908231125636\n",
      "episode 687, mean reward=-1.0,epsilon=0.25258514478483307\n",
      "episode 688, mean reward=-1.0,epsilon=0.252080476808122\n",
      "episode 689, mean reward=-1.0,epsilon=0.2515768171637373\n",
      "episode 690, mean reward=-1.0,epsilon=0.25107416383701997\n",
      "episode 691, mean reward=-1.0,epsilon=0.2505725148173357\n",
      "episode 692, mean reward=-1.0,epsilon=0.2500718680980679\n",
      "episode 693, mean reward=-1.0,epsilon=0.24957222167660875\n",
      "episode 694, mean reward=-1.0,epsilon=0.2490735735543521\n",
      "episode 695, mean reward=-1.0,epsilon=0.24857592173668494\n",
      "episode 696, mean reward=-1.0,epsilon=0.24807926423297907\n",
      "episode 697, mean reward=-1.0,epsilon=0.2475835990565844\n",
      "episode 698, mean reward=-1.0,epsilon=0.24708892422481935\n",
      "episode 699, mean reward=-1.0,epsilon=0.2465952377589643\n",
      "episode 700, mean reward=-1.0,epsilon=0.24610253768425297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 701, mean reward=-1.0,epsilon=0.24561082202986478\n",
      "episode 702, mean reward=-1.0,epsilon=0.24512008882891667\n",
      "episode 703, mean reward=-1.0,epsilon=0.2446303361184556\n",
      "episode 704, mean reward=-1.0,epsilon=0.2441415619394505\n",
      "episode 705, mean reward=-1.0,epsilon=0.2436537643367844\n",
      "episode 706, mean reward=-1.0,epsilon=0.24316694135924677\n",
      "episode 707, mean reward=-1.0,epsilon=0.24268109105952562\n",
      "episode 708, mean reward=-1.0,epsilon=0.24219621149419973\n",
      "episode 709, mean reward=-1.0,epsilon=0.2417123007237305\n",
      "episode 710, mean reward=-1.0,epsilon=0.24122935681245508\n",
      "episode 711, mean reward=-1.0,epsilon=0.24074737782857783\n",
      "episode 712, mean reward=-1.0,epsilon=0.24026636184416275\n",
      "episode 713, mean reward=-1.0,epsilon=0.23978630693512618\n",
      "episode 714, mean reward=-1.0,epsilon=0.2393072111812284\n",
      "episode 715, mean reward=-1.0,epsilon=0.23882907266606693\n",
      "episode 716, mean reward=-1.0,epsilon=0.23835188947706784\n",
      "episode 717, mean reward=-1.0,epsilon=0.23787565970547858\n",
      "episode 718, mean reward=-1.0,epsilon=0.2374003814463603\n",
      "episode 719, mean reward=-1.0,epsilon=0.23692605279858062\n",
      "episode 720, mean reward=-1.0,epsilon=0.23645267186480526\n",
      "episode 721, mean reward=-1.0,epsilon=0.23598023675149074\n",
      "episode 722, mean reward=-1.0,epsilon=0.23550874556887724\n",
      "episode 723, mean reward=-1.0,epsilon=0.23503819643098034\n",
      "episode 724, mean reward=-1.0,epsilon=0.23456858745558423\n",
      "episode 725, mean reward=-1.0,epsilon=0.2340999167642335\n",
      "episode 726, mean reward=-1.0,epsilon=0.23363218248222611\n",
      "episode 727, mean reward=-1.0,epsilon=0.2331653827386056\n",
      "episode 728, mean reward=-1.0,epsilon=0.23269951566615357\n",
      "episode 729, mean reward=-1.0,epsilon=0.23223457940138229\n",
      "episode 730, mean reward=-1.0,epsilon=0.23177057208452767\n",
      "episode 731, mean reward=-1.0,epsilon=0.2313074918595415\n",
      "episode 732, mean reward=-1.0,epsilon=0.2308453368740834\n",
      "episode 733, mean reward=-1.0,epsilon=0.23038410527951494\n",
      "episode 734, mean reward=-1.0,epsilon=0.22992379523088996\n",
      "episode 735, mean reward=-1.0,epsilon=0.22946440488694952\n",
      "episode 736, mean reward=-1.0,epsilon=0.22900593241011336\n",
      "episode 737, mean reward=-1.0,epsilon=0.2285483759664724\n",
      "episode 738, mean reward=-1.0,epsilon=0.22809173372578198\n",
      "episode 739, mean reward=-1.0,epsilon=0.22763600386145424\n",
      "episode 740, mean reward=-1.0,epsilon=0.22718118455055128\n",
      "episode 741, mean reward=-1.0,epsilon=0.2267272739737765\n",
      "episode 742, mean reward=-1.0,epsilon=0.2262742703154691\n",
      "episode 743, mean reward=-1.0,epsilon=0.2258221717635959\n",
      "episode 744, mean reward=-1.0,epsilon=0.22537097650974372\n",
      "episode 745, mean reward=-1.0,epsilon=0.22492068274911306\n",
      "episode 746, mean reward=-1.0,epsilon=0.22447128868051014\n",
      "episode 747, mean reward=-1.0,epsilon=0.22402279250634016\n",
      "episode 748, mean reward=-1.0,epsilon=0.22357519243259993\n",
      "episode 749, mean reward=-1.0,epsilon=0.2231284866688706\n",
      "episode 750, mean reward=-1.0,epsilon=0.22268267342831072\n",
      "episode 751, mean reward=-1.0,epsilon=0.22223775092764886\n",
      "episode 752, mean reward=-1.0,epsilon=0.22179371738717654\n",
      "episode 753, mean reward=-1.0,epsilon=0.22135057103074127\n",
      "episode 754, mean reward=-1.0,epsilon=0.22090831008573955\n",
      "episode 755, mean reward=-1.0,epsilon=0.2204669327831091\n",
      "episode 756, mean reward=-1.0,epsilon=0.2200264373573225\n",
      "episode 757, mean reward=-1.0,epsilon=0.21958682204637953\n",
      "episode 758, mean reward=-1.0,epsilon=0.21914808509180134\n",
      "episode 759, mean reward=-1.0,epsilon=0.21871022473862184\n",
      "episode 760, mean reward=-1.0,epsilon=0.21827323923538114\n",
      "episode 761, mean reward=-1.0,epsilon=0.2178371268341196\n",
      "episode 762, mean reward=-1.0,epsilon=0.2174018857903693\n",
      "episode 763, mean reward=-1.0,epsilon=0.21696751436314832\n",
      "episode 764, mean reward=-1.0,epsilon=0.21653401081495274\n",
      "episode 765, mean reward=-1.0,epsilon=0.21610137341175048\n",
      "episode 766, mean reward=-1.0,epsilon=0.21566960042297414\n",
      "episode 767, mean reward=-1.0,epsilon=0.21523869012151398\n",
      "episode 768, mean reward=-1.0,epsilon=0.21480864078371115\n",
      "episode 769, mean reward=-1.0,epsilon=0.21437945068935033\n",
      "episode 770, mean reward=-1.0,epsilon=0.21395111812165302\n",
      "episode 771, mean reward=-1.0,epsilon=0.2135236413672719\n",
      "episode 772, mean reward=-1.0,epsilon=0.21309701871628162\n",
      "episode 773, mean reward=-1.0,epsilon=0.21267124846217444\n",
      "episode 774, mean reward=-1.0,epsilon=0.2122463289018516\n",
      "episode 775, mean reward=-1.0,epsilon=0.21182225833561716\n",
      "episode 776, mean reward=-1.0,epsilon=0.21139903506717156\n",
      "episode 777, mean reward=-1.0,epsilon=0.21097665740360402\n",
      "episode 778, mean reward=-1.0,epsilon=0.21055512365538653\n",
      "episode 779, mean reward=-1.0,epsilon=0.21013443213636668\n",
      "episode 780, mean reward=-1.0,epsilon=0.20971458116376115\n",
      "episode 781, mean reward=-1.0,epsilon=0.2092955690581485\n",
      "episode 782, mean reward=-1.0,epsilon=0.20887739414346307\n",
      "episode 783, mean reward=-1.0,epsilon=0.20846005474698787\n",
      "episode 784, mean reward=-1.0,epsilon=0.208043549199348\n",
      "episode 785, mean reward=-1.0,epsilon=0.20762787583450423\n",
      "episode 786, mean reward=-1.0,epsilon=0.20721303298974605\n",
      "episode 787, mean reward=-1.0,epsilon=0.2067990190056845\n",
      "episode 788, mean reward=23.5,epsilon=0.20638583222624665\n",
      "episode 789, mean reward=21.5,epsilon=0.20597347099866842\n",
      "episode 790, mean reward=-1.0,epsilon=0.20556193367348788\n",
      "episode 791, mean reward=6.0,epsilon=0.2051512186045389\n",
      "episode 792, mean reward=16.5,epsilon=0.20474132414894355\n",
      "episode 793, mean reward=-1.0,epsilon=0.20433224866710764\n",
      "episode 794, mean reward=25.0,epsilon=0.20392399052271246\n",
      "episode 795, mean reward=-1.0,epsilon=0.20351654808270825\n",
      "episode 796, mean reward=18.5,epsilon=0.2031099197173081\n",
      "episode 797, mean reward=-1.0,epsilon=0.20270410379998238\n",
      "episode 798, mean reward=-1.0,epsilon=0.20229909870745025\n",
      "episode 799, mean reward=-1.0,epsilon=0.20189490281967454\n",
      "episode 800, mean reward=22.0,epsilon=0.20149151451985525\n",
      "episode 801, mean reward=23.5,epsilon=0.20108893219442267\n",
      "episode 802, mean reward=23.0,epsilon=0.2006871542330308\n",
      "episode 803, mean reward=23.5,epsilon=0.20028617902855073\n",
      "episode 804, mean reward=14.5,epsilon=0.1998860049770656\n",
      "episode 805, mean reward=23.0,epsilon=0.1994866304778621\n",
      "episode 806, mean reward=24.0,epsilon=0.19908805393342607\n",
      "episode 807, mean reward=24.5,epsilon=0.1986902737494347\n",
      "episode 808, mean reward=24.0,epsilon=0.19829328833475102\n",
      "episode 809, mean reward=16.0,epsilon=0.19789709610141687\n",
      "episode 810, mean reward=-1.0,epsilon=0.19750169546464688\n",
      "episode 811, mean reward=-1.0,epsilon=0.1971070848428224\n",
      "episode 812, mean reward=-1.0,epsilon=0.19671326265748423\n",
      "episode 813, mean reward=-1.0,epsilon=0.19632022733332757\n",
      "episode 814, mean reward=-1.0,epsilon=0.19592797729819508\n",
      "episode 815, mean reward=-1.0,epsilon=0.1955365109830702\n",
      "episode 816, mean reward=-1.0,epsilon=0.19514582682207154\n",
      "episode 817, mean reward=-1.0,epsilon=0.19475592325244623\n",
      "episode 818, mean reward=-1.0,epsilon=0.1943667987145638\n",
      "episode 819, mean reward=-1.0,epsilon=0.19397845165191013\n",
      "episode 820, mean reward=19.0,epsilon=0.19359088051108087\n",
      "episode 821, mean reward=-1.0,epsilon=0.19320408374177567\n",
      "episode 822, mean reward=-1.0,epsilon=0.19281805979679129\n",
      "episode 823, mean reward=-1.0,epsilon=0.192432807132016\n",
      "episode 824, mean reward=-1.0,epsilon=0.19204832420642307\n",
      "episode 825, mean reward=-1.0,epsilon=0.19166460948206493\n",
      "episode 826, mean reward=-1.0,epsilon=0.19128166142406716\n",
      "episode 827, mean reward=-1.0,epsilon=0.19089947850062114\n",
      "episode 828, mean reward=-1.0,epsilon=0.19051805918297982\n",
      "episode 829, mean reward=-1.0,epsilon=0.19013740194545012\n",
      "episode 830, mean reward=21.0,epsilon=0.1897575052653872\n",
      "episode 831, mean reward=-1.0,epsilon=0.18937836762318874\n",
      "episode 832, mean reward=-1.0,epsilon=0.18899998750228875\n",
      "episode 833, mean reward=-1.0,epsilon=0.1886223633891507\n",
      "episode 834, mean reward=-1.0,epsilon=0.18824549377326252\n",
      "episode 835, mean reward=-1.0,epsilon=0.18786937714713006\n",
      "episode 836, mean reward=-1.0,epsilon=0.18749401200627194\n",
      "episode 837, mean reward=-1.0,epsilon=0.18711939684921156\n",
      "episode 838, mean reward=-1.0,epsilon=0.18674553017747303\n",
      "episode 839, mean reward=-1.0,epsilon=0.1863724104955739\n",
      "episode 840, mean reward=-1.0,epsilon=0.18600003631102033\n",
      "episode 841, mean reward=-1.0,epsilon=0.18562840613429973\n",
      "episode 842, mean reward=-1.0,epsilon=0.18525751847887667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 843, mean reward=-1.0,epsilon=0.18488737186118503\n",
      "episode 844, mean reward=-1.0,epsilon=0.18451796480062288\n",
      "episode 845, mean reward=-1.0,epsilon=0.18414929581954664\n",
      "episode 846, mean reward=-1.0,epsilon=0.18378136344326548\n",
      "episode 847, mean reward=-1.0,epsilon=0.18341416620003442\n",
      "episode 848, mean reward=-1.0,epsilon=0.18304770262104916\n",
      "episode 849, mean reward=-1.0,epsilon=0.18268197124044036\n",
      "episode 850, mean reward=1.5,epsilon=0.18231697059526755\n",
      "episode 851, mean reward=-1.0,epsilon=0.18195269922551285\n",
      "episode 852, mean reward=18.5,epsilon=0.18158915567407588\n",
      "episode 853, mean reward=22.5,epsilon=0.18122633848676756\n",
      "episode 854, mean reward=-1.0,epsilon=0.1808642462123039\n",
      "episode 855, mean reward=-1.0,epsilon=0.18050287740230078\n",
      "episode 856, mean reward=-1.0,epsilon=0.1801422306112683\n",
      "episode 857, mean reward=-1.0,epsilon=0.179782304396604\n",
      "episode 858, mean reward=23.0,epsilon=0.17942309731858838\n",
      "episode 859, mean reward=-1.0,epsilon=0.17906460794037798\n",
      "episode 860, mean reward=8.0,epsilon=0.1787068348280009\n",
      "episode 861, mean reward=-1.0,epsilon=0.17834977655034964\n",
      "episode 862, mean reward=20.0,epsilon=0.1779934316791766\n",
      "episode 863, mean reward=21.0,epsilon=0.17763779878908723\n",
      "episode 864, mean reward=15.0,epsilon=0.1772828764575356\n",
      "episode 865, mean reward=-1.0,epsilon=0.17692866326481735\n",
      "episode 866, mean reward=20.0,epsilon=0.17657515779406538\n",
      "episode 867, mean reward=-1.0,epsilon=0.1762223586312432\n",
      "episode 868, mean reward=22.5,epsilon=0.17587026436513953\n",
      "episode 869, mean reward=23.0,epsilon=0.17551887358736293\n",
      "episode 870, mean reward=22.5,epsilon=0.17516818489233532\n",
      "episode 871, mean reward=20.5,epsilon=0.1748181968772877\n",
      "episode 872, mean reward=16.0,epsilon=0.17446890814225333\n",
      "episode 873, mean reward=21.0,epsilon=0.17412031729006297\n",
      "episode 874, mean reward=-1.0,epsilon=0.17377242292633877\n",
      "episode 875, mean reward=21.0,epsilon=0.1734252236594889\n",
      "episode 876, mean reward=-1.0,epsilon=0.17307871810070202\n",
      "episode 877, mean reward=22.0,epsilon=0.17273290486394152\n",
      "episode 878, mean reward=24.0,epsilon=0.1723877825659401\n",
      "episode 879, mean reward=24.0,epsilon=0.17204334982619462\n",
      "episode 880, mean reward=23.0,epsilon=0.17169960526695938\n",
      "episode 881, mean reward=20.5,epsilon=0.1713565475132421\n",
      "episode 882, mean reward=-1.0,epsilon=0.17101417519279774\n",
      "episode 883, mean reward=-1.0,epsilon=0.1706724869361227\n",
      "episode 884, mean reward=-1.0,epsilon=0.1703314813764502\n",
      "episode 885, mean reward=-1.0,epsilon=0.16999115714974375\n",
      "episode 886, mean reward=22.0,epsilon=0.16965151289469196\n",
      "episode 887, mean reward=-1.0,epsilon=0.16931254725270417\n",
      "episode 888, mean reward=-1.0,epsilon=0.16897425886790382\n",
      "episode 889, mean reward=23.0,epsilon=0.1686366463871232\n",
      "episode 890, mean reward=21.0,epsilon=0.16829970845989842\n",
      "episode 891, mean reward=21.5,epsilon=0.1679634437384638\n",
      "episode 892, mean reward=24.0,epsilon=0.16762785087774695\n",
      "episode 893, mean reward=21.5,epsilon=0.1672929285353622\n",
      "episode 894, mean reward=-1.0,epsilon=0.16695867537160639\n",
      "episode 895, mean reward=22.5,epsilon=0.16662509004945306\n",
      "episode 896, mean reward=-1.0,epsilon=0.16629217123454723\n",
      "episode 897, mean reward=18.5,epsilon=0.1659599175952\n",
      "episode 898, mean reward=-1.0,epsilon=0.16562832780238268\n",
      "episode 899, mean reward=-1.0,epsilon=0.16529740052972253\n",
      "episode 900, mean reward=-1.0,epsilon=0.16496713445349698\n",
      "episode 901, mean reward=18.0,epsilon=0.16463752825262773\n",
      "episode 902, mean reward=-1.0,epsilon=0.1643085806086766\n",
      "episode 903, mean reward=-1.0,epsilon=0.16398029020583943\n",
      "episode 904, mean reward=-1.0,epsilon=0.16365265573094104\n",
      "episode 905, mean reward=-1.0,epsilon=0.16332567587343003\n",
      "episode 906, mean reward=-1.0,epsilon=0.16299934932537333\n",
      "episode 907, mean reward=19.5,epsilon=0.16267367478145145\n",
      "episode 908, mean reward=16.0,epsilon=0.16234865093895245\n",
      "episode 909, mean reward=11.0,epsilon=0.1620242764977676\n",
      "episode 910, mean reward=8.0,epsilon=0.1617005501603858\n",
      "episode 911, mean reward=11.5,epsilon=0.16137747063188831\n",
      "episode 912, mean reward=18.0,epsilon=0.16105503661994355\n",
      "episode 913, mean reward=18.5,epsilon=0.16073324683480247\n",
      "episode 914, mean reward=18.0,epsilon=0.16041209998929246\n",
      "episode 915, mean reward=-1.0,epsilon=0.16009159479881288\n",
      "episode 916, mean reward=7.5,epsilon=0.15977172998132966\n",
      "episode 917, mean reward=7.0,epsilon=0.1594525042573701\n",
      "episode 918, mean reward=-1.0,epsilon=0.1591339163500184\n",
      "episode 919, mean reward=14.0,epsilon=0.1588159649849096\n",
      "episode 920, mean reward=-1.0,epsilon=0.15849864889022525\n",
      "episode 921, mean reward=-1.0,epsilon=0.15818196679668795\n",
      "episode 922, mean reward=-1.0,epsilon=0.15786591743755612\n",
      "episode 923, mean reward=-1.0,epsilon=0.15755049954861916\n",
      "episode 924, mean reward=-1.0,epsilon=0.15723571186819257\n",
      "episode 925, mean reward=-1.0,epsilon=0.15692155313711248\n",
      "episode 926, mean reward=-1.0,epsilon=0.15660802209873106\n",
      "episode 927, mean reward=-1.0,epsilon=0.1562951174989113\n",
      "episode 928, mean reward=-1.0,epsilon=0.15598283808602162\n",
      "episode 929, mean reward=-1.0,epsilon=0.15567118261093182\n",
      "episode 930, mean reward=-1.0,epsilon=0.1553601498270069\n",
      "episode 931, mean reward=-1.0,epsilon=0.1550497384901029\n",
      "episode 932, mean reward=-1.0,epsilon=0.15473994735856156\n",
      "episode 933, mean reward=-0.5,epsilon=0.15443077519320575\n",
      "episode 934, mean reward=-1.0,epsilon=0.15412222075733398\n",
      "episode 935, mean reward=-1.0,epsilon=0.15381428281671564\n",
      "episode 936, mean reward=-1.0,epsilon=0.1535069601395861\n",
      "episode 937, mean reward=-0.5,epsilon=0.15320025149664226\n",
      "episode 938, mean reward=-1.0,epsilon=0.15289415566103673\n",
      "episode 939, mean reward=-1.0,epsilon=0.15258867140837346\n",
      "episode 940, mean reward=-1.0,epsilon=0.1522837975167027\n",
      "episode 941, mean reward=-1.0,epsilon=0.15197953276651663\n",
      "episode 942, mean reward=-1.0,epsilon=0.15167587594074358\n",
      "episode 943, mean reward=-1.0,epsilon=0.15137282582474346\n",
      "episode 944, mean reward=-1.0,epsilon=0.15107038120630345\n",
      "episode 945, mean reward=-1.0,epsilon=0.15076854087563246\n",
      "episode 946, mean reward=-1.0,epsilon=0.15046730362535704\n",
      "episode 947, mean reward=-1.0,epsilon=0.15016666825051556\n",
      "episode 948, mean reward=-1.0,epsilon=0.14986663354855384\n",
      "episode 949, mean reward=-1.0,epsilon=0.14956719831932092\n",
      "episode 950, mean reward=-1.0,epsilon=0.1492683613650634\n",
      "episode 951, mean reward=-1.0,epsilon=0.14897012149042124\n",
      "episode 952, mean reward=11.0,epsilon=0.14867247750242243\n",
      "episode 953, mean reward=-1.0,epsilon=0.1483754282104789\n",
      "episode 954, mean reward=13.5,epsilon=0.14807897242638118\n",
      "episode 955, mean reward=-1.0,epsilon=0.14778310896429378\n",
      "episode 956, mean reward=13.5,epsilon=0.1474878366407506\n",
      "episode 957, mean reward=21.5,epsilon=0.1471931542746503\n",
      "episode 958, mean reward=-1.0,epsilon=0.1468990606872509\n",
      "episode 959, mean reward=19.5,epsilon=0.1466055547021663\n",
      "episode 960, mean reward=25.0,epsilon=0.14631263514536025\n",
      "episode 961, mean reward=19.5,epsilon=0.1460203008451424\n",
      "episode 962, mean reward=-1.0,epsilon=0.14572855063216347\n",
      "episode 963, mean reward=23.0,epsilon=0.1454373833394105\n",
      "episode 964, mean reward=-1.0,epsilon=0.1451467978022025\n",
      "episode 965, mean reward=24.5,epsilon=0.14485679285818512\n",
      "episode 966, mean reward=-1.0,epsilon=0.1445673673473265\n",
      "episode 967, mean reward=-1.0,epsilon=0.14427852011191283\n",
      "episode 968, mean reward=20.5,epsilon=0.14399024999654336\n",
      "episode 969, mean reward=7.0,epsilon=0.14370255584812555\n",
      "episode 970, mean reward=-1.0,epsilon=0.14341543651587063\n",
      "episode 971, mean reward=2.5,epsilon=0.14312889085128994\n",
      "episode 972, mean reward=23.0,epsilon=0.14284291770818858\n",
      "episode 973, mean reward=18.0,epsilon=0.14255751594266225\n",
      "episode 974, mean reward=22.5,epsilon=0.14227268441309207\n",
      "episode 975, mean reward=22.0,epsilon=0.14198842198014028\n",
      "episode 976, mean reward=20.0,epsilon=0.14170472750674554\n",
      "episode 977, mean reward=21.5,epsilon=0.14142159985811759\n",
      "episode 978, mean reward=6.0,epsilon=0.1411390379017344\n",
      "episode 979, mean reward=16.0,epsilon=0.14085704050733694\n",
      "episode 980, mean reward=18.0,epsilon=0.14057560654692366\n",
      "episode 981, mean reward=-1.0,epsilon=0.1402947348947469\n",
      "episode 982, mean reward=-1.0,epsilon=0.14001442442730883\n",
      "episode 983, mean reward=20.0,epsilon=0.13973467402335563\n",
      "episode 984, mean reward=13.5,epsilon=0.13945548256387427\n",
      "episode 985, mean reward=16.5,epsilon=0.1391768489320875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 986, mean reward=20.0,epsilon=0.1388987720134489\n",
      "episode 987, mean reward=21.0,epsilon=0.13862125069563977\n",
      "episode 988, mean reward=15.5,epsilon=0.1383442838685631\n",
      "episode 989, mean reward=22.5,epsilon=0.13806787042434024\n",
      "episode 990, mean reward=17.5,epsilon=0.1377920092573061\n",
      "episode 991, mean reward=17.0,epsilon=0.13751669926400426\n",
      "episode 992, mean reward=20.5,epsilon=0.13724193934318354\n",
      "episode 993, mean reward=21.5,epsilon=0.1369677283957929\n",
      "episode 994, mean reward=20.0,epsilon=0.1366940653249772\n",
      "episode 995, mean reward=21.0,epsilon=0.1364209490360728\n",
      "episode 996, mean reward=19.0,epsilon=0.1361483784366037\n",
      "episode 997, mean reward=20.0,epsilon=0.13587635243627538\n",
      "episode 998, mean reward=-1.0,epsilon=0.1356048699469731\n",
      "episode 999, mean reward=-1.0,epsilon=0.13533392988275578\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    episodes_n=1000\n",
    "    weights_path=\"D:/RL_CartPole_agent_weights/MountainCarContinuous_dqn2.h5\"\n",
    "    env=gym.make(\"MountainCar-v0\")\n",
    "    state_size=env.observation_space.shape[0]\n",
    "    action_size=env.action_space.n\n",
    "    \n",
    "    agent=DQNAgent(state_size,action_size)#,path=\"D:/RL_CartPole_agent_weights/MountainCarContinuous_dqn.h5\")\n",
    "    #agent.load_weights()\n",
    "    max_pos=-0.4\n",
    "    success=[]\n",
    "    steps=200\n",
    "    \n",
    "    \n",
    "    for e in range(episodes_n):\n",
    "        current_reward=0\n",
    "        state=env.reset()\n",
    "        if (e%10==0):\n",
    "            agent.update_target_model()\n",
    "        #done=False\n",
    "        #while(not done):\n",
    "        for step in range(steps):\n",
    "            if (agent.render):\n",
    "                env.render()\n",
    "            \n",
    "            action=agent.eps_greedy_policy(np.reshape(state,(1,state_size)))\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            # Adjust reward based on car position\n",
    "            if (next_state[0]>max_pos):\n",
    "                max_pos=next_state[0]\n",
    "                reward+=10\n",
    "            else:\n",
    "                reward=reward\n",
    "            if (next_state[0]>=0.5):\n",
    "                reward+=100\n",
    "                success.append(e)\n",
    "            current_reward+=reward\n",
    "       \n",
    "            agent.memorize_sample(state,action,reward,next_state,done)\n",
    "            agent.train_model()\n",
    "            state=next_state\n",
    "        print(\"episode {}, mean reward={},epsilon={}\".format(e,current_reward/200,agent.epsilon))\n",
    "        if e % 10 == 0:\n",
    "            agent.model.save_weights(weights_path)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 303\n",
      "Trainable params: 303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"MountainCar-v0\")\n",
    "state_size=env.observation_space.shape[0]\n",
    "action_size=env.action_space.n\n",
    "agent=DQNAgent(state_size,action_size,path=\"D:/RL_CartPole_agent_weights/MountainCarDiscrete_dqn.h5\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()\n",
    "state=np.reshape(state,(1,agent.state_size))\n",
    "while(state[0][0]<0.5):\n",
    "        env.render()\n",
    "        action=agent.greedy_policy(state)\n",
    "        next_state,reward,done,_=env.step(np.array(action))\n",
    "        next_state=np.reshape(next_state,(1,state_size))\n",
    "        state=next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
